---
title: 'Before Stochastic Differential Equations'
date: Oct 06, 2025
author: Hardik Prabhu
image: 

tags:
  - SDE
  - Measure Theory
---
When I first read [Øksendal's *Stochastic Differential Equations*](https://link.springer.com/book/10.1007/978-3-642-14394-6), I kept having to backtrack to piece together various concepts. Here's what I wish I'd reviewed beforehand, it'll make the book much more approachable.

## Random Variables

Given a probability space $(\Omega, \mathcal{F}, P)$, a **random variable** is a measurable mapping $X: \Omega \to \mathbb{R}^n$ such that for all $B \in \mathcal{B}_n$ (the Borel σ-algebra on $\mathbb{R}^n$), we have:

$$
X^{-1}(B) \in \mathcal{F}
$$

In some texts, the term "random variable" is reserved for mappings $X: \Omega \to \mathbb{R}$, while "random vector" is used for mappings to $\mathbb{R}^n$ with $n > 1$. We will not make this distinction here and use "random variable" for both cases.

**Note:** If $X = (X_1, X_2, \ldots, X_n): \Omega \to \mathbb{R}^n$ is a random variable, then each component $X_i: \Omega \to \mathbb{R}$ is itself a random variable.

This is possible because every linear transformation is Borel measurable, and if $X$ is Borel measurable, then the composition is Borel measurable.

In fact, the converse is true as well, stacking two random variables will give us another random variable.

### σ-Algebra Generated by Random Variables

The σ-algebra generated by a random variable $X$, denoted $\sigma(X)$, is the smallest σ-algebra on $\Omega$ that makes $X$ measurable. Formally:

$$
\sigma(X) = \{X^{-1}(B) : B \in \mathcal{B}_n\}
$$

This σ-algebra captures all the information about $X$ that can be observed. Knowledge of $\sigma(X)$ is sufficient to determine the distribution of $X$, including all moments, probabilities, and other distributional properties.

### Generated σ-Algebras and Preimages

For any function $f$ (could be non-measurable) mapping from $X$ to $Y$ of measurable spaces $(X, \mathcal{S})$, $(Y, \mathcal{T})$:

$$
\sigma(f) = \{f^{-1}(B) : B \in \mathcal{T}\}
$$

is the same as:

$$
\sigma(\{f^{-1}(B) : B \in \mathcal{G}\})
$$

where $\sigma(\mathcal{G}) = \mathcal{T}$.

The σ-algebra generated by $f$ (all preimages of measurable sets in $\mathcal{T}$) equals the σ-algebra generated by just the preimages of a generating set $\mathcal{G}$ for $\mathcal{T}$.

**Notation:** $\sigma(\text{generating set})$ denotes the σ-algebra generated by that set.

This allows us more flexibility with how we define the σ-algebra generated by a random variable. Typically we map to Borel σ-algebra in Euclidean space, which is a product space, and we can use the set of rectangles in the case of product spaces as the generating sets. Further, we can even consider the set of half-boxes (intervals) as generating sets.

### Alternative Representations

In conclusion, for a random variable $X: \Omega \to \mathbb{R}^n$:

$$
\sigma(X) = \{X^{-1}(B): B \in \mathcal{B}_n\}
$$

or equivalently:

$$
\sigma(X) = \sigma(\{X^{-1}(B_1 \times B_2 \times \cdots \times B_n): B_i \in \mathcal{B}_1 \text{ (Borel in } \mathbb{R})\})
$$

or:

$$
\sigma(X) = \sigma(\{X^{-1}((-\infty, b_1] \times (-\infty, b_2] \times \cdots \times (-\infty, b_n]): b_i \in \mathbb{R}\})
$$

Since we can break the random variables into the components $X = (X_1, X_2, \ldots, X_n)$ where $X_i: \Omega \to \mathbb{R}$ are the component random variables, we can write:

$$
X^{-1}(B_1 \times B_2 \times \cdots \times B_n) = X_1^{-1}(B_1) \cap X_2^{-1}(B_2) \cap \cdots \cap X_n^{-1}(B_n)
$$

Therefore:

$$
\sigma(X) = \sigma\left(\{X_1^{-1}(B_1) \cap X_2^{-1}(B_2) \cap \cdots \cap X_n^{-1}(B_n): B_i \in \mathcal{B}_1\}\right)
$$

which interestingly equals:

$$
\sigma(X) = \sigma\left(\bigcup_{i=1}^{n} \{X_i^{-1}(B): B \in \mathcal{B}_1\}\right) = \sigma\left(\bigcup_{i=1}^{n} \sigma(X_i)\right)
$$

## Collections of Random Variables

### Finite Collections

For any finite collection of random variables $\{X_1, X_2, \ldots, X_n\}$, we can treat them as a single random variable $X = (X_1, \ldots, X_n): \Omega \to \mathbb{R}^n$, and the σ-algebra generated by the collection is:

$$
\sigma(X_1, X_2, \ldots, X_n) = \sigma(X)
$$

### Infinite Collections and Cylinder Sets

When the index set is not finite (for example, in a continuous stochastic process where we take $[0, \infty)$ as the time index), we need a different approach. Consider a collection of random variables $\{X_t : t \in T\}$ where $T$ is an arbitrary (possibly uncountable) index set.

The σ-algebra generated by this collection is:

$$
\sigma(\{X_t : t \in T\}) = \sigma\left(\bigcup_{t \in T} \sigma(X_t)\right)
$$

However, working directly with this definition is often difficult. Instead, we use **cylinder sets** as a generating set.

**Cylinder Sets:** For a finite subset $\{t_1, t_2, \ldots, t_n\} \subseteq T$ and Borel sets $B_1, B_2, \ldots, B_n \in \mathcal{B}_1$, a **cylinder set** is defined as:

$$
C_{t_1, \ldots, t_n}(B_1, \ldots, B_n) = \{\omega \in \Omega : X_{t_1}(\omega) \in B_1, X_{t_2}(\omega) \in B_2, \ldots, X_{t_n}(\omega) \in B_n\}
$$

$$
= X_{t_1}^{-1}(B_1) \cap X_{t_2}^{-1}(B_2) \cap \cdots \cap X_{t_n}^{-1}(B_n)
$$

The collection of all cylinder sets forms a generating set for $\sigma(\{X_t : t \in T\})$:

$$
\sigma(\{X_t : t \in T\}) = \sigma(\{\text{all cylinder sets}\})
$$

More precisely:

$$
\sigma(\{X_t : t \in T\}) = \sigma\left(\left\{\bigcap_{i=1}^{n} X_{t_i}^{-1}(B_i) : n \in \mathbb{N}, t_1, \ldots, t_n \in T, B_i \in \mathcal{B}_1\right\}\right)
$$

## Doob-Dynkin Lemma

Since we have had a detailed conversation regarding the sigma algebra generated by a random variable. Its natural to consider what if another random variable is measurable with respect to it.

Let (Ω, F, P) be a probability space, and let X: Ω → S and Y: Ω → T be random variables, where S and T are measurable spaces.

**Statement:** The following are equivalent:

1. **Y is σ(X)-measurable**, i.e., Y is measurable with respect to σ(X)
2. **Y = f(X)** for some measurable function f: S → T

In other words: **Y is a measurable function of X if and only if Y is measurable with respect to the σ-algebra generated by X**.

# Stochastic Process

A **stochastic process** is a collection of random variables $\{X_t : t \in T\}$ defined on a probability space $(\Omega, \mathcal{F}, P)$ and indexed by a set $T$. The index set $T$ is typically interpreted as time and can be:

- **Discrete:** $T = \{0, 1, 2, \ldots\}$ or $T = \mathbb{Z}$
- **Continuous:** $T = [0, \infty)$ or $T = \mathbb{R}$

For each fixed $t \in T$, $X_t: \Omega \to \mathbb{R}$ (or $\mathbb{R}^n$) is a random variable. For each fixed $\omega \in \Omega$, the mapping $t \mapsto X_t(\omega)$ is called a **sample path** or **trajectory** of the process.

Note that if we consider Borel sets in range $[0,\infty)$ (denoted $\mathcal{B}_T$) for $T$, then $X(t,\omega) = X_t(\omega)$ is a product space measurable map, and hence the stochastic process is simply a random variable from $(T \times \Omega, \mathcal{B}_T \otimes \mathcal{F}, P')$. 

In practice, we are not concerned  P' or for that matter P is. This is because while we are tempted to model any real-world phenomenon as a stochastic process, we should recognize that we often don't explicitly know the underlying sample space $\Omega$. In practice, we work with the observed values of the process and their distributions, rather than the abstract probability space itself.

## The Trajectory Space

As discussed above, modeling a real world phenomenon as a stochastic process directly may not be feasible. The sample space is hard to define and especially challenging in the case the range of the variable is uncountable. All we can assume is that we can make observation at various time steps.
As making infinite observations is also kind of impractical. By observation, I meant for any time index $t$, and associated random variable, we can see $X_t(w)$ for an unknown $w$.

By fixing a $\omega \in \Omega$, we can create a trajectory function $\phi_\omega: T \to \mathbb{R}$ defined by:
$$\phi_\omega(t) = X_t(\omega)$$

Since, we don't know what the sample space is, we can consider $(\mathbb{R}^N)^T$ as the set of trajectories (functions mapping from $T \to \mathbb{R}^N$) as a canonical sample space.
The reason behind this is no matter what the sample space is, we can transfer the modelling to the canonical space.

Also notice that the mapping $w \to \phi_w $ need not be one to one as two different sample points may result in same observations. For example, in a die throw, let the random variables be 1 when even number is facing upwards, and 0 when its odd. One might argue that in case the probability mass is not uniform and observing "2" maybe more likely  than "4", are we modelling this incorrectly?

The answer is **no**. What matters is not the original sample space $\Omega$, but the **induced probability measure** on the canonical space $(\mathbb{R}^N)^T$. Since, we dont get to "observe" whether its 2 or 4. It's like someone else sees the experiment and then tells us whether its even or odd. All we can observe is the random variable $X(w)$! 
This is actually a good thing, we dont have to truly understand the nature's generative process in order to approximately generate from it. 

The next immediate thing to ask is what is the sigma algebra for the canonical space.

Well, for any random variable $X$, as long as we know the $\sigma(X)$ and the probability measure on it, no information is lost.
So whatever, sigma algbera we consider must atleast contain $\simga(X)$.


