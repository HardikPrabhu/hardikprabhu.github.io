---
title: 'Introduction to Generative Modeling'
date: Aug 26, 2023
author: Hardik Prabhu

tags:
  - Generative modeling
  - Probability
  
---


This blog provides a primer on generative modeling, summarizing the knowledge I've gathered up to this point. It will serve as a foundational piece that I plan to build upon and update regularly. As I delve deeper into the subject, I intend to create more comprehensive articles focusing on various subtopics.

## Motivation 

Generative modeling, promotes understanding of causual relationships between variables involved as we try to understand how to generate an outcome. By mimicking the underlying generative process, this approach allows us to not only replicate outcomes but also unravel the interplay between variables and their influences. 

Following is a paragraph from ["An Introduction to Variational Autoencoders"](https://arxiv.org/pdf/1906.02691.pdf) \[1\].

 >"A generative model simulates how the data is generated in the real world. “Modeling” is understood in almost every science as unveiling this generating process by hypothesizing theories and testing these theories through observations. For instance, when meteorologists model the weather they use highly complex partial differential equations to express the underlying physics of the weather. Or when an astronomer models the formation of galaxies s/he encodes in his/her equations of motion the physical laws under which stellar bodies interact. The same is true for biologists, chemists, economists and so on. Modeling in the sciences is in fact almost always generative modeling".                 

From a probablistic viewpoint, data is generated by an underlying distribution $p_{data}$. Generative modeling requires us to model the distribution of the data $p_{\theta}$. This endeavor empowers us to deduce the probability of a specific dataset occurrence and, of even greater significance, provides a means to draw samples effectively generating outcomes adhering to the characteristics of the data distribution.

The generative model is parameterised by $p_{\theta}$. Our objective then becomes to minimize the "distance" between $p_{data}$ and $p_{\theta}$.

$$ \theta^{*} = \argmin_{\theta}{dist(p_{data},p_{theta})} $$


While various concepts of distances are present, the Kullback-Leibler ($D_{KL}$) divergence stands out as a widely embraced measure. It is formally given by the following equation.

$$ D_{\text{KL}}(P \| Q) = \mathbb{E}_{x \sim P} \left[ \log \left( \frac{P(x)}{Q(x)} \right) \right] $$

 Subsequently, we will explore alternate distance metrics in use. A rationale supporting its prominence stems from the requirement that approximating KL divergence involves sampling from the data distribution. 
 
 Let $D = \lbrace x^{(1)},x^{(2)},...x^{(n)} \rbrace$ be an  i.i.d dataset sampled from $p_{data}$. Then the $D_{KL}$ between the distibutions could be approximated as:

 $$D_{\text{KL}}(p_{data} \| p_{\theta}) \approx \frac{1}{n} \sum_{i=1}^{n}  \log \left( \frac{p(x^{(i)})}{p_{\theta}(x^{(i)})} \right) $$

This is useful because, in order to optimize $\theta$, we could remove the numerator term.

$ \theta^{*} = \argmin_{\theta}{\frac{1}{n} \sum_{i=1}^{n}  \log \left( \frac{p(x^{(i)})}{p_{\theta}(x^{(i)})} \right)} $

$ \Rightarrow \theta^{*} = \argmin_{\theta}{\frac{1}{n} \sum_{i=1}^{n}  \log \left( \frac{p(x^{(i)})}{p_{\theta}(x^{(i)})} \right)} $


$ \Rightarrow \theta^{*} = \argmin_{\theta}{\sum_{i=1}^{n}  \log ( p(x^{(i)})) - 
 \sum_{i=1}^{n}log({p_{\theta}(x^{(i)})})} $

$ \Rightarrow \theta^{*} = \argmax_{\theta}{ 
 \sum_{i=1}^{n}log({p_{\theta}(x^{(i)})})} $ 

 Hence, minimizing KLD is same as maximizing likleyhood of the sampled dataset given the parameter $\theta$.

 ## Asymmetric nature of the KL divergence 

The asymmetric nature of the Kullback-Leibler (KL) divergence is a fundamental characteristic that distinguishes it from other distance metrics. KL divergence measures the difference between two probability distributions \(P\) and \(Q\) in a non-symmetric manner.

This asymmetry is evident in the formula for KL divergence:

$$ D_{\text{KL}}(P \| Q) = \int P(x) \log \left( \frac{P(x)}{Q(x)} \right) dx $$


We find it convinient for measuring the divergence as $D_{\text{KL}}(p_{data} \| p_{\theta})$. This way we don't need to explicitly know the distribution $p_data$ that we trying to model. 


Before moving ahead lets, focus on what the differenet ordring entails. Lets take a look at an illustration given in ["NIPS 2016 Tutorial:Generative Adversarial Networks"]("https://arxiv.org/pdf/1701.00160.pdf") [2].

<figure>
	<img src="/images/blogs_img/gen_intro/image.png">
	<figcaption> Fig 1 : Effect of Difference of order in KL. Goal is to try optimize the distribution "q" given distribution "p". </figcaption>
</figure>


Before making any remarks, it is important to understand the context of the image presented. The data distribution shown in blue (p) is bi-modal, whereas the capacity of the model distribution shown in green is limited to be an univariate gaussian. The distribution of q is limited, just as our model would be limited in terms of the actual complexity of any real data that we are trying to learn. As summarized  by Goodfellow \[2\] :

>"We can think of $D_{KL}(p_{data} \| p_{model})$ as preferring to place high probability everywhere that the data occurs, and $D_{KL}(p_{model} \| p_{data})$ as preferrring to place low probability where data doesnt occur."

Looking at it from a sampling standpoint, an argument could be made in favor of utilizing KL in the opposite direction. This is because it ensures the generation of samples that appear realistic, but there's a risk that these samples might only originate from a specific mode within the distribution, a phenomenon known as "mode collapse." This approach might not be effective in generating data that covers a wide range of high-density regions. Moreover, as we've observed previously, its not possible to explicitly write $p_{data}(x)$. So optimzing parameter of our model will become tricky. 


## We trained a generative model. Now What?

  We will see later on the different ways to design generative models. We can ask three core questions for assessing the efficacy of such models:

1. $\textbf{Density Estimation}$: When presented with a specific datapoint $x$, the question arises: what probability is attributed to it by the model, denoted as $p_{\theta}(x)$?

2. $\textbf{Sampling}$: The inquiry into generating fresh, previously unseen data from the model distribution comes to the fore: how can we generate novel data points, represented as $x_{\text{new}} \sim p_{\theta}(x)$?

3. $\textbf{Unsupervised Representation Learning}$ : There are some models such as Variational autoencoders (VAEs) which assumes a hierachical data generation process, where first some latent variable (repesentation) is generated and then the data is generated by conditioning  on it.  From such models, we would like to uncover significant feature representations for a given datapoint $x$. How do we acquire meaningful features that capture the essence of $x$ in an unsupervised manner? Such models bridge a connection between representation learning and generative modeling.





## References

1. Kingma, D.P. and Welling, M., 2019. An introduction to variational autoencoders. Foundations and Trends® in Machine Learning, 12(4), pp.307-392.

2. Goodfellow, I., 2016. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160.



