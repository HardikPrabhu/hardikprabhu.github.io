---
title: 'Introduction to Generative Modeling'
date: Aug 26, 2023
author: Hardik Prabhu

tags:
  - Generative modeling
  - Probability
  - KL Divergence
  - Likelyhood
  
---

This blog provides a primer on generative modeling, summarizing introductory knowledge required before venturing into more advanced topics. It will serve as a foundational piece that I plan to build upon and update whenever necessary. As I delve deeper into the subject, I intend to create more comprehensive articles focusing on various topics, This blog acts as an initial stepping stone towards that goal.

## Motivation 

Generative modeling, promotes understanding of causual relationships between variables involved as we try to understand how their   values culminate to generate a certain outcome. By mimicking the underlying generative process, this approach allows us to not only replicate outcomes but also unravel the interplay between variables involved and their influences. 

Following is a paragraph from ["An Introduction to Variational Autoencoders"](https://arxiv.org/pdf/1906.02691.pdf) \[1\].

 >"A generative model simulates how the data is generated in the real world. “Modeling” is understood in almost every science as unveiling this generating process by hypothesizing theories and testing these theories through observations. For instance, when meteorologists model the weather they use highly complex partial differential equations to express the underlying physics of the weather. Or when an astronomer models the formation of galaxies s/he encodes in his/her equations of motion the physical laws under which stellar bodies interact. The same is true for biologists, chemists, economists and so on. Modeling in the sciences is in fact almost always generative modeling".                 

From a probablistic viewpoint, data is generated by an underlying distribution $p_{data}$. Generative modeling requires us to model the distribution of the data $p_{\theta}$. This endeavor empowers us to deduce the probability of a specific dataset occurrence and, of even greater significance, provides a means to draw samples effectively generating outcomes adhering to the characteristics of the data distribution.


### Problems we would like to solve using generative models

* Data Generation : Synthesizing image, video, speech, text and more.

* Data Compression and meaningful representation : In a generative process, the data is assumed to be generated in a hirerachy, first concepts are generated in a concept (latent) space. Data compression and meaningful representations revolve around the idea of creating efficient mappings between the real and the latent space. 

* Anomaly Detection : It is done through either likelyhood estimatation or reconstruction error. 



## Probablistic modeling

The generative model is parameterised by $p_{\theta}$. Our objective then becomes to minimize the "distance" between $p_{data}$ and $p_{\theta}$.

$$ \theta^{*} =  argmin_{\theta}  
 \text{  } dist(p_{data},p_{theta}) $$


While various concepts of distances are present, the Kullback-Leibler ($D_{KL}$) divergence stands out as a widely embraced measure. It is formally given by the following equation.

$$ D_{\text{KL}}(P \| Q) = \mathbb{E}_{x \sim P} \left[ \log \left( \frac{P(x)}{Q(x)} \right) \right] $$

 Subsequently, we will explore alternate distance metrics in use. A rationale supporting its prominence stems from the requirement that approximating KL divergence involves sampling from the data distribution. 
 
 Let $D = \lbrace x^{(1)},x^{(2)},...x^{(n)} \rbrace$ be an  i.i.d dataset sampled from $p_{data}$. Then the $D_{KL}$ between the distibutions could be approximated as:

 $$D_{\text{KL}}(p_{data} \| p_{\theta}) \approx \frac{1}{n} \sum_{i=1}^{n}  \log \left( \frac{p(x^{(i)})}{p_{\theta}(x^{(i)})} \right) $$

This is useful because, in order to optimize $\theta$, we could remove the numerator term.

$ \theta^{*} = argmin_{\theta} \frac{1}{n} \sum_{i=1}^{n}  \log \left( \frac{p(x^{(i)})}{p_{\theta}(x^{(i)})} \right) $

$ \Rightarrow \theta^{*} = argmin_{\theta} \frac{1}{n} \sum_{i=1}^{n}  \log \left( \frac{p(x^{(i)})}{p_{\theta}(x^{(i)})} \right)$


$ \Rightarrow \theta^{*} = argmin_{\theta}\sum_{i=1}^{n}  \log ( p(x^{(i)})) - 
 \sum_{i=1}^{n}log({p_{\theta}(x^{(i)})}) $

$ \Rightarrow \theta^{*} = argmax_{\theta}  
 \sum_{i=1}^{n}log({p_{\theta}(x^{(i)})}) $ 

 Hence, minimizing KLD is same as maximizing likleyhood of the sampled dataset given the parameters$\theta$.We used a sample mean to approximate the expectation, and this type of estimation is also referred to as Monte Carlo estimation. We will revisit Monte Carlo estimates at a later point. For now, let's delve deeper into the analysis of the KL divergence.


## Asymmetric nature of the KL divergence 

The asymmetric nature of the Kullback-Leibler (KL) divergence is a fundamental characteristic that distinguishes it from other distance metrics. KL divergence measures the difference between two probability distributions \(P\) and \(Q\) in a non-symmetric manner.

This asymmetry is evident in the formula for KL divergence:

$$ D_{\text{KL}}(P \| Q) = \int P(x) \log \left( \frac{P(x)}{Q(x)} \right) dx $$


We find it convinient for measuring the divergence as $D_{\text{KL}}(p_{data} \| p_{\theta})$. This way we don't need to explicitly know the distribution $p_data$ that we trying to model. But, for a while lets focus on what the different ordering entails. Lets take a look at an illustration given in ["NIPS 2016 Tutorial:Generative Adversarial Networks"]("https://arxiv.org/pdf/1701.00160.pdf") \[2\].

<figure>
	<img src="/images/blogs_img/gen_intro/image.png">
	<figcaption> Fig 1 : Effect of difference of order in KL. Goal is to try optimize the distribution "q" given distribution "p". </figcaption>
</figure>


Before making any remarks, it is important to understand the context of the image presented. The data distribution shown in blue (p) is bi-modal, whereas the capacity of the model distribution shown in green is limited to be an univariate gaussian. The distribution of q is limited, just as our model would be limited in terms of the actual complexity of any real data that we are trying to learn. As summarized  by Goodfellow \[2\] :

>"We can think of $D_{KL}(p_{data} \| p_{model})$ as preferring to place high probability everywhere that the data occurs, and $D_{KL}(p_{model} \| p_{data})$ as preferrring to place low probability where data doesnt occur."

Looking at it from a sampling standpoint, an argument could be made in favor of utilizing KL in the opposite direction. This is because it ensures the generation of samples that appear realistic, but there's a risk that these samples might only originate from a specific mode within the distribution, a phenomenon known as "mode collapse." This approach might not be effective in generating data that covers a wide range of high-density regions. Moreover, as we've observed previously, its not possible to explicitly write $p_{data}(x)$. Therfore, optimzing parameters of our model will become tricky. 


## Efficacy of a generative model

  We will see later on the different ways to design generative models. We can ask three core questions for assessing the efficacy of such models:

1. $\textbf{Density Estimation}$: When presented with a specific datapoint $x$,  what probability density function value $p_{\theta}(x)$ is attributed to it by the model ?

2. $\textbf{Sampling}$: How can we generate fresh, previously unseen data points from the model distribution, represented as $x_{\text{new}} \sim p_{\theta}(x)$? 

3. $\textbf{Unsupervised Representation Learning}$ : There are some models such as Variational autoencoders (VAEs) which assumes a hierachical data generation process, where first some latent variable (repesentation) is generated and then the data is generated by conditioning  on it.  From such models, we would like to uncover significant feature representations for a given datapoint $x$. How do we acquire meaningful features that capture the essence of $x$ in an unsupervised manner? Such models bridge a connection between representation learning and generative modeling.


## Simple parametric model



Simplest example that I could think of is of a univariate gaussian distribution. It has two parameters mean $\mu$ and variance $\sigma^2$. Denoted by $x \sim N( x ;\mu,\sigma^2)$. Then the probability density is given as follows:

$$ p_\theta(x) = \frac{1}{\sqrt{2 \pi}\sigma} e^{-\frac{(x-\mu)^2}{\sigma^2}}  $$

Assume we have a dataset of  independent  samples $\lbrace x_1, x_2, \ldots, x_n \rbrace$ from the Gaussian distribution. The likelihood function is:

$$L(\mu, \sigma | x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)
$$

The log-likelihood function is:
$$
\log L(\mu, \sigma | x_1, x_2, \ldots, x_n) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2
$$

The maximum likelihood estimate for $\mu$ is obtained by differentiating the log-likelihood with respect to $\mu$ and setting it to zero:
$$
\frac{\partial}{\partial \mu} \log L = \frac{1}{\sigma^2} \sum_{i=1}^{n} (x_i - \mu) = 0
$$
Solving for $\mu$:
$$
\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i
$$

The maximum likelihood estimate for $\sigma^2$ is obtained by differentiating the log-likelihood with respect to $\sigma^2$ and setting it to zero:
$$
\frac{\partial}{\partial (\sigma^2)} \log L = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^{n} (x_i - \mu)^2 = 0
$$
Solving for $\sigma^2$:
$$
\hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2
$$



As we can see, in situations involving a straightforward parametric model, it's easy to perform the parameter optimization by maximizing the likelihood of the training data. Nevertheless, circumstances don't always permit us to observe all the variables. Consequently, when dealing with more sophisticated latent variable models, we may need to employ alternative methods like the EM-Algorithm or Variational Inference which I will discuss in a seperate blog. 


## Equipping with some useful concepts

I would like to finish this blog with some useful concepts which would come in handy in our future discussions.

### 1. Probabilistic chain rule

$$P(X_1, X_2, \ldots, X_n) = P(X_1) \cdot P(X_2 | X_1) \cdot P(X_3 | X_1, X_2) \cdot \ldots \cdot P(X_n | X_1, X_2, \ldots, X_{n-1})$$

### 2. Monte-carlo estimation

### 3. Gradient of expectation

### 4.Transforming random variables

### 5. Markov assumption
























## References

1. Kingma, D.P. and Welling, M., 2019. An introduction to variational autoencoders. Foundations and Trends® in Machine Learning, 12(4), pp.307-392.

2. Goodfellow, I., 2016. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160.



