---
title: 'Measure and Probability Theory Essentials'
date: Dec 22, 2024
author: Hardik Prabhu
image: 

tags:
  - Mathematics
  - Probability
  - Measure Theory
  
---

This blog focuses on key concepts from the books that I found valuable. While I will generally omit proofs, I'll point out notable insights that are essential. For understanding them in detail, I encourage reading the books themselves and working out the chapter exercises. The goal is to keep track of all the essential theorems and ideas required for generative modelling

The books are available here:

[1] https://link.springer.com/book/10.1007/978-981-13-6678-9 (Kesavan)

[2] https://link.springer.com/book/10.1007/978-1-4471-0645-6 (Capinski)


# Rings, Algebras, Sigma Algebras

Before talking more concretely about measures, we should have a consistent notion of what we are allowed to measure. Not everything is going to be measurable. Intuitively, given a set of entities, such as a collection of objects, we would like to have some uniform way to compare the different collections and assign them meaningful numerical values that reflect their size or extent a.k.a measure. It could be as simple as counting the number of elements in the set. Or the length, area or volume a set of points covers in Euclidean space.

When dealing with collections of objects or sets in a space, we often want to perform operations on these sets and still be able to measure the results. Two fundamental operations we frequently encounter are unions and differences of sets.

For unions, we want our measure to be additive in some sense. If we have two non-overlapping sets A and B, we would expect the measure of their union $(A \cup B)$ to be the sum of their measures. For set differences, we want our measure to reflect the intuitive notion of "subtracting" one set from another. we would then expect the measure of their difference to be the measure of A minus the measure of B.

Therefore, the resulting sets obtained by performing these fundamental operations should be measurable. This leads us to define the fundamental collection of subsets of a space, called a ring.

**Definition** A ring is a collection of subsets on which a measure could be defined.

* Closed under union: If $A$ and $B$ are in $\mathcal{R}$, then $A \cup B$ is also in $\mathcal{R}$
* Closed under set difference: If $A$ and $B$ are in $\mathcal{R}$, then $A \setminus B$ is also in $\mathcal{R}$
* The empty set $\emptyset$ is in $\mathcal{R}$

**Properties** The following properties can be derived from the above definition:

* Closed under finite unions: By using the principle of mathematical induction, we can show that if $A_1, A_2, ..., A_n$ are in $\mathcal{R}$, then $\bigcup_{i=1}^n A_i$ is also in $\mathcal{R}$
* Closed under finite intersections: For any $A, B \in \mathcal{R}$, we have $A \cap B = A \setminus (A \setminus B)$. Since $\mathcal{R}$ is closed under set difference, $A \cap B \in \mathcal{R}$. This can be extended to finite intersections by induction
* Closed under symmetric difference: For any $A, B \in \mathcal{R}$, the symmetric difference $A \triangle B = (A \setminus B) \cup (B \setminus A)$ is in $\mathcal{R}$, as $\mathcal{R}$ is closed under both set difference and union

**Definition** An algebra is a ring of subsets of $X$, which also contains the set $X$.

The most trivial example of an algebra is the power set $\mathcal{P}(X)$ of $X$.

**Definition** A $\sigma$-ring $\mathcal{S}$ on a set $X$ is a ring of subsets of $X$ that is closed under countable unions as well.

If $$ \{A_n\}_{n=1}^{\infty} $$ is a countable sequence of sets in $$ \mathcal{S} $$, then $$ \bigcup_{n=1}^{\infty} A_n \in \mathcal{S} $$.


**Properties** The following properties describe how a ring is generated from an arbitrary collection of subsets of $X$. Let $\varepsilon \subset \mathcal{P}(X)$. Then:

* The collection of subsets of $X$ which can be covered by the union of finite members of $\varepsilon$ is a ring:
  $$\mathcal{R}_\varepsilon = \{A \subseteq X : A \subseteq \bigcup_{i=1}^n E_i \text{ for some finite } n \in \mathbb{N} \text{ and } E_1, ..., E_n \in \mathcal{E}\}$$
* If we replace finite union with countable then $\mathcal{R}_\varepsilon$ is a $\sigma$-ring
* $\mathcal{R}_\varepsilon$ contains $\varepsilon$
* Let $${\mathcal{R}_\alpha}_{\alpha \in I}$$ be a collection of rings on a set $X$. Then $$\bigcap_{\alpha \in I} \mathcal{R}_\alpha$$ is also a ring on $X$
* Hence, the smallest ring containing $\varepsilon$ is the intersection of all the rings containing it
* Therefore, all the members of the smallest ring $\mathcal{R}(\varepsilon)$, also known as the ring generated by $\varepsilon$ can be covered by some finite union of members of $\varepsilon$
* Extending the same notion to $\sigma$-ring, $\mathcal{S}(\varepsilon)$ is the smallest sigma ring containing $\varepsilon$. And it is the intersection of all the $\sigma$-rings containing it. Moreover, all its elements can be covered by a countable union of members of $\varepsilon$

# Measures on rings

Rings form the mathematical foundation for what we can measure, with sigma algebras being particularly important. For example in probability theory, the measure is defined for the sigma-algebra containing subsets of a sample space.

**Definition 4.** A measure, $\mu$ on the ring $R$ is an extended real-valued function such that:

* $\mu(E) \geq 0$ for all $E \in R$
* $\mu(\emptyset) = 0$
* $\mu$ is countably additive. If $$ {E_n}_{n=1}^{\infty} $$ is a sequence of disjoint sets in $R$ such that $$ \bigcup_{n=1}^{\infty} E_n \in R $$, then:
  $\mu(\bigcup_{n=1}^{\infty} E_n) = \sum_{n=1}^{\infty} \mu(E_n)$

**Properties** 
* Monotonicity: For any sets $A, B \in R$, if $A \subseteq B$, then:
$\mu(A) \leq \mu(B)$

* Countable Subadditivity: For any sequence of sets $$ {E_n}_{n=1}^{\infty} $$ in $R$ where $$ \bigcup_{n=1}^{\infty} E_n \in R$$ :
$$ \mu(\bigcup_{n=1}^{\infty} E_n) \leq \sum_{n=1}^{\infty} \mu(E_n) $$


* Continuity from Below (for increasing sequences):
If $$\{E_n\}{n=1}^{\infty}$$ is an increasing sequence of sets in R (meaning $$E_1 \subseteq E_2 \subseteq E_3 \subseteq \cdots$$) and $$\bigcup_{n=1}^{\infty} E_n \in R$$, then:
$\mu(\bigcup_{n=1}^{\infty} E_n) = \lim_{n \to \infty} \mu(E_n)$

* Continuity from Above (for decreasing sequences):
If $$\{E_n\}{n=1}^{\infty}$$ is a decreasing sequence of sets in R (meaning $$E_1 \supseteq E_2 \supseteq E_3 \supseteq \cdots$$) and $$\mu(E_1) < \infty$$, and $$\bigcap_{n=1}^{\infty} E_n \in R$$, then:
$$\mu(\bigcap_{n=1}^{\infty} E_n) = \lim_{n \to \infty} \mu(E_n)$$

These properties are crucial because they ensure that measures behave well with respect to limits of sets. The continuity from above property requires the additional condition that $$\mu(E_1) < \infty$$. No such condition is needed for continuity from below since measures are non-negative.

**Finite Measure**: A measure $\mu$ on a ring $R$ is called finite if there exists a set $E \in R$ such that $\mu(E) < \infty$ and every set in $R$ is contained in $E$.

**Sigma-Finite Measure**: A measure $\mu$ on a ring $R$ is called sigma-finite if there exists a countable collection of sets $\{E_n\}_{n=1}^{\infty}$ in $R$ such that:

-	$\mu(E_n) < \infty$ for all $n \geq 1$
-	Every set in $R$ is contained in $\bigcup_{n=1}^{\infty} E_n$
In other words, the ring can be covered by countably many sets of finite measure.

**Complete Measure**: A measure $\mu$ on a sigma-algebra $\Sigma$ is called complete if whenever $E \in \Sigma$ with $\mu(E) = 0$, then every subset $F \subseteq E$ also belongs to $\Sigma$ (and consequently $\mu(F) = 0$).


# The Borel Sigma-Algebra

The **Borel sigma-algebra** is one of the most fundamental sigma-algebras and forms the backbone for defining a lot of popular measures in Euclidean spaces, such as probabilty distributions of random variables, length, area and volumne and many more.


- In $\mathbb{R}$, it is denoted by $B_1$. More generally, in $\mathbb{R}^n$, it is denoted by $B_n$.
- It is generated by the **open sets** in $\mathbb{R}^n$. An equivalent way of generating it is through intervals (in $\mathbb{R}$) or boxes (in $\mathbb{R}^n$), often taken as half-open sets.

A ring of sets $R$ on $\mathbb{R}^n$ can be constructed from finite unions of **half-open boxes** (also called half-open intervals in $\mathbb{R}^1$), specifically sets of the form $(a_1, b_1] \times (a_2, b_2] \times \cdots \times (a_n, b_n]$ where the $a_i, b_i$ are real numbers with $a_i < b_i$ for all $i$.

$B_n = \sigma(R)$  is  the smallest sigma-algebra containing all half-open boxes is called the **Borel sigma-algebra** on $\mathbb{R}^n$. 

Instead of defining it directly as a sigma algebra of open sets, and following the approach of first construction of a ring then generating a sigma algebra from that Ring is a powerful idea. The CarathÃ©odory extension theorem extends a measure from a ring (or semiring) to a sigma-algebra. This stepwise approach makes formulation and proof techniques manageable and rigorous.
Furthermore, we started with intervals (boxes), which themselves are pi-systems which will be useful to show uniqness in measures.

**Equivalent Definitions**

- Sigma algebra generated by open sets is the same as sigma algebra generated by the collection of half-open boxes.

- Sigma algebra generated by the ring containing finite unions of half-open boxes is the same as sigma algebra directly generated by the collection of half-open boxes.

- Ring containing finite unions of half open boxes is the same as ring containing finite disjoint unions of half open boxes.


# Probability as a Measure

Probability is defined as a measure on a sigma alebra of sample space. When we conduct a random experiment, we begin with the sample space $\Omega$ (omega), which contains all possible outcomes of the experiment. A sigma algebra $\mathcal{F}$ contains events which are subsets of the sample space. A event is said to occur when the observation (outcome) of the random experiment belongs to the event. 

A collection of such events forms a $\sigma$-algebra (sigma-algebra) $\mathcal{F}$ of subsets of $\Omega$, which satisfies three key properties:

1. The sample space $\Omega$ itself belongs to $\mathcal{F}$
2. If an event $A$ belongs to $\mathcal{F}$, then its complement $A^c$ also belongs to $\mathcal{F}$
3. The union of any countable sequence of events in $\mathcal{F}$ also belongs to $\mathcal{F}$

Probability $P$ is defined as a measure that satisfies the following axioms:

1. Non-negativity: For any event $A \in \mathcal{F}$, $P(A) \geq 0$
2. $P(\Omega) = 1$
3. Countable additivity: For any sequence of disjoint events ${A_n}$ in $\mathcal{F}$, $P(\bigcup A_n) = \sum P(A_n)$

This measure-theoretic approach provides the mathematical framework necessary for handling both discrete and continuous probability distributions within a unified theory. It allows us to rigorously define concepts like random variables, independence, and conditional probability.


# Uniquness of a mesure (sigma-finite)


A powerful concept in measure theory is that to prove uniqueness of a measure defined on a sigma-algebra generated by a simpler collection of sets, it suffices to verify uniqueness on that initial collection. This is formalized by the **pi-lambda theorem**.

### Pi-System

A **pi-system** ($\pi$-system) is a non-empty collection of subsets that is closed under finite intersection.
Formally, a class $\mathcal{P}$ of subsets of a set $X$ is a pi-system if for any $A, B \in \mathcal{P}$, we have $A \cap B \in \mathcal{P}$.

Example: The collection of all intervals (including half-open or open) in $\mathbb{R}$ forms a pi-system.


### Lambda-System

A **lambda-system** ($\lambda$-system, also called d-system or Dynkin system), is a collection $\Lambda$ of subsets of a set $X$ such that:

1. $X \in \Lambda$
2. If $A, B \in \Lambda$ and $A \subseteq B$, then $B \setminus A \in \Lambda$
3. If $A_1, A_2, \ldots \in \Lambda$ are disjoint and their union is also in $\Lambda$, then $\bigcup_{n=1}^\infty A_n \in \Lambda$

### Pi-Lambda Theorem and Uniqueness of sigma-finite measures

The **pi-lambda theorem** (Dynkinâs theorem) states that if $\mathcal{P}$ is a pi-system and $\Lambda$ is a lambda-system with $\mathcal{P} \subseteq \Lambda$, then the sigma-algebra generated by $\mathcal{P}$, denoted $\sigma(\mathcal{P})$, is contained in $\Lambda$:

$$
\sigma(\mathcal{P}) \subseteq \Lambda
$$


In order to show uniqueness we will use the fact that any ring is a Pi-system and the collection of subsets where the measure is unique is a lambda system, provided that the measure is sigma-finite. Therefore the sigma algebra generated must be contained by the lambda system.

Hence, to prove uniqueness of two measures defined on $\sigma(\mathcal{P})$, it is enough to check that they agree on the pi-system $\mathcal{P}$.

***

# Measurable Functions and Integration

## Measurable Functions

When working with different measure spaces, we need a way to transform elements from one space to another while preserving the measurability property. This is where measurable functions come into play.

**Definition:** Let $(X, \Sigma)$ and $(Y, \mathcal{T})$ be measurable spaces, meaning that $X$ and $Y$ are sets equipped with respective $\sigma$-algebras $\Sigma$ and $\mathcal{T}$. A function $f : X \to Y$ is said to be **measurable** if for every $E \in \mathcal{T}$ the pre-image of $E$ under $f$ is in $\Sigma$; that is, for all $E \in \mathcal{T}$:

$$f^{-1}(E) := \{x \in X \mid f(x) \in E\} \in \Sigma$$

In other words, $f$ is measurable if $\sigma(f) \subseteq \Sigma$, where $\sigma(f)$ is the $\sigma$-algebra generated by $f$.

When we want to emphasize the dependency on the $\sigma$-algebras, we write:

$$f : (X, \Sigma) \to (Y, \mathcal{T})$$

In many applications, particularly in probability and analysis, $\mathcal{T}$ is often taken as the Borel sigma algebra on $\mathbb{R}^n$, which is the sigma algebra generated by all open sets in $\mathbb{R}^n$.

For defining integrals in measure theory settings which extends Riemann integral to more general settings, we assume $\mathcal{T}$ is Borel sigma algebra on $\mathbb{R}$. 

This gives rise to an equivalent condition for definition:

 $f^{-1}(a,\infty) \in \Sigma$ for any $a \in \mathbb{R}$.

### Properties of Measurable Functions (Range belongs to $\mathbb{R}$)

Measurable functions have several important properties that make them powerful tools in measure theory:

1. The sum and product of two measurable functions is measurable.
2. The composition $f \circ g$ of measurable functions is measurable.
3. If $f$ is measurable, then $$|f|$$, $\max(0,f(x))$, and $\max(0,-f(x))$ are all measurable.
4. The point-wise supremum and infimum of a sequence of measurable functions is measurable.
5. The liminf, limsup, and limit (all taken pointwise) of a sequence of measurable functions is a measurable function.

These properties ensure that measurable functions form a robust class that's closed under many common operations, making them ideal for real analysis.

## Simple Functions: Building Blocks for Integration

A fundamental concept in measure theory is the simple function, which serves as a building block for defining integration.

**Definition:** Let $(X, \Sigma)$ be a measurable space. A **simple function** is a function $f:X\to\mathbb{R}$ of the form:

$$f(x)=\sum_{k=1}^{n}a_k \mathbf{1}_{A_k}(x)$$

where $A_1, \ldots, A_n \in \Sigma$ are disjoint measurable sets, $a_1, \ldots, a_n$ are real  numbers, and $\mathbf{1}_A$ is the indicator function of the set $A$.

Simple functions can be thought of as step functions that take only a finite number of values. They're particularly important because of the following theorem:

**Theorem:** If $f$ is a positive measurable function, then there exists an increasing sequence of simple functions converging pointwise to it.

This theorem is crucial for integration theory as it allows us to approximate any positive measurable function using simple functions, for which integration is straightforward to define.

## Integration in Measure Theory

Integration in measure theory extends the familiar Riemann integral to more general settings. We build it in stages:

### Integration of Simple Functions

Let $(X, \Sigma, \mu)$ be a measure space and $f : X \rightarrow [0, \infty]$ be a simple function of the form $f = \sum_{k=1}^{n} a_k \mathbf{1}_{A_k}$ where $A_1, \ldots, A_n \in \Sigma$ are disjoint measurable sets.

The integral of $f$ with respect to the measure $\mu$ is defined as:

$$\int_X f \, d\mu = \sum_{k=1}^{n} a_k \mu(A_k)$$

If $E \in \Sigma$, then the integral of $f$ over $E$ is defined as:

$$\int_E f \, d\mu = \int_X f \mathbf{1}_E \, d\mu = \sum_{k=1}^{n} a_k \mu(A_k \cap E)$$

This definition is intuitive: we weight each value of the function by the measure of the set on which it takes that value.

### Integration of Positive Measurable Functions

For a general positive measurable function $f: X \to [0, \infty]$, we use the approximation theorem mentioned earlier. Since we can approximate $f$ by an increasing sequence of simple functions, we define:

$$\int_X f \, d\mu = \sup_{0 \leq \phi \leq f} \int_X \phi \, d\mu$$

where the supremum is taken over all simple functions $\phi$ such that $0 \leq \phi \leq f$.

### Integration of Positive Measurable Functions

For a general positive measurable function $f: X \to [0, \infty]$, we use the approximation theorem mentioned earlier. Since we can approximate $f$ by an increasing sequence of simple functions, we define:

$$\int_X f \, d\mu = \sup_{0 \leq \phi \leq f} \int_X \phi \, d\mu$$

where the supremum is taken over all simple functions $\phi$ such that $0 \leq \phi \leq f$.

### Integration of Integrable Functions

While the definition for positive measurable functions provides a foundation, we need to extend integration to handle functions that take both positive and negative values (or even complex values). We approach this by decomposing a general function into its positive and negative parts.

For any real-valued measurable function $f$, we can define:
- The positive part: $f^+(x) = \max(f(x), 0)$
- The negative part: $f^-(x) = \max(-f(x), 0)$

Note that both $f^+$ and $f^-$ are non-negative measurable functions, and $f = f^+ - f^-$.

**Definition:** A measurable function $f: X \to \mathbb{R}$ is said to be **integrable** if both $\int_X f^+ \, d\mu < \infty$ and $\int_X f^- \, d\mu < \infty$. 

For an integrable function $f$, we define its integral as:

$$\int_X f \, d\mu = \int_X f^+ \, d\mu - \int_X f^- \, d\mu$$

This approach is sometimes called the Lebesgue integral. It's worth noting that this definition ensures that the integral is well-defined (not $\infty - \infty$) because we require both parts to be finite.

The space of all integrable functions with respect to measure $\mu$ is commonly denoted as $L^1(X, \mu)$ or simply $L^1$ when the context is clear.

#### Extension to Complex-Valued Functions

For complex-valued measurable functions $f: X \to \mathbb{C}$, we decompose $f$ into its real and imaginary parts:

$$f(x) = \text{Re}(f(x)) + i\text{Im}(f(x))$$

A complex-valued function $f$ is integrable if both its real and imaginary parts are integrable, and we define:

$$\int_X f \, d\mu = \int_X \text{Re}(f) \, d\mu + i\int_X \text{Im}(f) \, d\mu$$

#### Some Properties of the Integral

The Lebesgue integral has several important properties:

1. **Linearity**: For integrable functions $f$ and $g$ and constants $\alpha$ and $\beta$:
   $$\int_X (\alpha f + \beta g) \, d\mu = \alpha \int_X f \, d\mu + \beta \int_X g \, d\mu$$

2. **Monotonicity**: If $f$ and $g$ are integrable and $f(x) \leq g(x)$ for almost all $x \in X$, then:
   $$\int_X f \, d\mu \leq \int_X g \, d\mu$$

3. **Absolute value inequality**: If $f$ is integrable, then:
   $$\left|\int_X f \, d\mu\right| \leq \int_X |f| \, d\mu$$


## Convergence Theorems in Integration 

A central question in integration theory is: When can we interchange limits and integrals? That is, under what conditions does $\lim_{n\to\infty} \int f_n \, d\mu = \int \lim_{n\to\infty} f_n \, d\mu$? The following theorems provide answers to this question in various settings.

### 1. Monotone Convergence Theorem

**Theorem (Monotone Convergence):** Let $(X, \Sigma, \mu)$ be a measure space. If $\{f_n\}$ is an increasing sequence of measurable functions (i.e., $0 \leq f_1 \leq f_2 \leq \ldots$) that converges pointwise to a function $f$, then:

$$\lim_{n \to \infty} \int_X f_n \, d\mu = \int_X f \, d\mu$$

This theorem allows us to interchange the limit and integral operations when dealing with an increasing sequence of non-negative functions.


### 2. Beppo Levi's Theorem (Series Version of MCT)

**Theorem (Beppo Levi):** Let $(X, \Sigma, \mu)$ be a measure space and let $\{f_n\}$ be a sequence of non-negative measurable functions. Then:

$$\int_X \sum_{n=1}^{\infty} f_n \, d\mu = \sum_{n=1}^{\infty} \int_X f_n \, d\mu$$

This theorem is essentially an application of the Monotone Convergence Theorem to series. It states that the integral of a sum equals the sum of the integrals, provided all functions are non-negative.

### 3. Fatou's Lemma

**Lemma (Fatou):** Let $(X, \Sigma, \mu)$ be a measure space and let $\{f_n\}$ be a sequence of non-negative measurable functions. Then:

$$\int_X \liminf_{n \to \infty} f_n \, d\mu \leq \liminf_{n \to \infty} \int_X f_n \, d\mu$$

Fatou's Lemma essentially says that the integral of the limit inferior is less than or equal to the limit inferior of the integrals. It provides a one-sided bound when we don't have monotonicity or domination.

### 4. Dominated Convergence Theorem

**Theorem (Dominated Convergence):** Let $(X, \Sigma, \mu)$ be a measure space. If $\{f_n\}$ is a sequence of measurable functions that converges pointwise almost everywhere to a function $f$, and there exists an integrable function $g$ such that $$\vert f_n(x)\vert \leq g(x)$$ for all $n$ and almost all $x \in X$, then:

1. $f$ is integrable
2. $\lim_{n \to \infty} \int_X f_n \, d\mu = \int_X f \, d\mu$

This powerful theorem gives conditions under which we can interchange limits and integrals even for functions that take both positive and negative values.



[Pending]

## Product Spaces 

## Random Variables and Vectors

## Lp spaces and inequalities

## Raydon-Nikodym, Signed Measures and Conditional Expectations



