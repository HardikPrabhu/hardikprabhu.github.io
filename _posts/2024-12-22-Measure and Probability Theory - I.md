---
title: 'Measure and Probability Theory - I'
date: Dec 22, 2024
author: Hardik Prabhu
image: 

tags:
  - Mathematics
  - Probability
  - Measure Theory
  
---

These blogs focus on key concepts from two reference books that I found valuable. While I will generally omit proofs, I'll point out notable techniques and insights that are essential for understanding certain proofs. For a detailed understanding, I encourage reading the books themselves and working out the chapter exercises.

The books are available here:

[1] https://link.springer.com/book/10.1007/978-981-13-6678-9 (Kesavan)

[2] https://link.springer.com/book/10.1007/978-1-4471-0645-6 (Capinski)


# Rings, Algebras, Sigma Algebras

Before talking more concretely about measures, we should have a consistent notion of what we are allowed to measure. Not everything is going to be measurable. Intuitively, given a set of entities, such as a collection of objects, we would like to have some uniform way to compare the different collections and assign them meaningful numerical values that reflect their size or extent a.k.a measure. It could be as simple as counting the number of elements in the set. Or the length, area or volume a set of points covers in Euclidean space.

When dealing with collections of objects or sets in a space, we often want to perform operations on these sets and still be able to measure the results. Two fundamental operations we frequently encounter are unions and differences of sets.

For unions, we want our measure to be additive in some sense. If we have two non-overlapping sets A and B, we would expect the measure of their union $(A \cup B)$ to be the sum of their measures. For set differences, we want our measure to reflect the intuitive notion of "subtracting" one set from another. we would then expect the measure of their difference to be the measure of A minus the measure of B.

Therefore, the resulting sets obtained by performing these fundamental operations should be measurable. This leads us to define the fundamental collection of subsets of a space, called a ring.

**Definition 1.** A ring is a collection of subsets on which a measure could be defined.

* Closed under union: If $A$ and $B$ are in $\mathcal{R}$, then $A \cup B$ is also in $\mathcal{R}$
* Closed under set difference: If $A$ and $B$ are in $\mathcal{R}$, then $A \setminus B$ is also in $\mathcal{R}$
* The empty set $\emptyset$ is in $\mathcal{R}$

**Properties 1.** The following properties can be derived from the above definition:

* Closed under finite unions: By using the principle of mathematical induction, we can show that if $A_1, A_2, ..., A_n$ are in $\mathcal{R}$, then $\bigcup_{i=1}^n A_i$ is also in $\mathcal{R}$
* Closed under finite intersections: For any $A, B \in \mathcal{R}$, we have $A \cap B = A \setminus (A \setminus B)$. Since $\mathcal{R}$ is closed under set difference, $A \cap B \in \mathcal{R}$. This can be extended to finite intersections by induction
* Closed under symmetric difference: For any $A, B \in \mathcal{R}$, the symmetric difference $A \triangle B = (A \setminus B) \cup (B \setminus A)$ is in $\mathcal{R}$, as $\mathcal{R}$ is closed under both set difference and union

**Definition 2.** An algebra is a ring of subsets of $X$, which also contains the set $X$.

The most trivial example of an algebra is the power set $\mathcal{P}(X)$ of $X$.

**Definition 3.** A $\sigma$-ring $\mathcal{S}$ on a set $X$ is a ring of subsets of $X$ that is closed under countable unions as well.

If $$ {A_n}_{n=1}^{\infty} $$ is a countable sequence of sets in $$ \mathcal{S} $$, then $$ \bigcup_{n=1}^{\infty} A_n \in \mathcal{S} $$.

Similarly, $\sigma$-algebra is closed under countable unions.

**Properties 2.** The following properties describe how a ring is generated from an arbitrary collection of subsets of $X$. Let $\varepsilon \subset \mathcal{P}(X)$. Then:

* The collection of subsets of $X$ which can be covered by the union of finite members of $\varepsilon$ is a ring:
  $$\mathcal{R}_\varepsilon = \{A \subseteq X : A \subseteq \bigcup_{i=1}^n E_i \text{ for some finite } n \in \mathbb{N} \text{ and } E_1, ..., E_n \in \mathcal{E}\}$$
* If we replace finite union with countable then $\mathcal{R}_\varepsilon$ is a $\sigma$-ring
* $\mathcal{R}_\varepsilon$ contains $\varepsilon$
* Let $${\mathcal{R}_\alpha}_{\alpha \in I}$$ be a collection of rings on a set $X$. Then $$\bigcap_{\alpha \in I} \mathcal{R}_\alpha$$ is also a ring on $X$
* Hence, the smallest ring containing $\varepsilon$ is the intersection of all the rings containing it
* Therefore, all the members of the smallest ring $\mathcal{R}(\varepsilon)$, also known as the ring generated by $\varepsilon$ can be covered by some finite union of members of $\varepsilon$
* Extending the same notion to $\sigma$-ring, $\mathcal{S}(\varepsilon)$ is the smallest sigma ring containing $\varepsilon$. And it is the intersection of all the $\sigma$-rings containing it. Moreover, all its elements can be covered by a countable union of members of $\varepsilon$

# Measures on rings

Rings form the mathematical foundation for what we can measure, with sigma algebras being particularly important. For example in probability theory, the measure is defined for the sigma-algebra containing subsets of a sample space.

**Definition 4.** A measure, $\mu$ on the ring $R$ is an extended real-valued function such that:

* $\mu(E) \geq 0$ for all $E \in R$
* $\mu(\emptyset) = 0$
* $\mu$ is countably additive. If $$ {E_n}_{n=1}^{\infty} $$ is a sequence of disjoint sets in $R$ such that $$ \bigcup_{n=1}^{\infty} E_n \in R $$, then:
  $\mu(\bigcup_{n=1}^{\infty} E_n) = \sum_{n=1}^{\infty} \mu(E_n)$

**Properties 1.** Monotonicity: For any sets $A, B \in R$, if $A \subseteq B$, then:
$\mu(A) \leq \mu(B)$

**Properties 2.** Countable Subadditivity: For any sequence of sets $$ {E_n}_{n=1}^{\infty} $$ in $R$ where $$ \bigcup_{n=1}^{\infty} E_n \in R $$ :
$$ \mu(\bigcup_{n=1}^{\infty} E_n) \leq \sum_{n=1}^{\infty} \mu(E_n) $$


**Properties 3.** Continuity from Below (for increasing sequences):
If $$\{E_n\}{n=1}^{\infty}$$ is an increasing sequence of sets in R (meaning $$E_1 \subseteq E_2 \subseteq E_3 \subseteq \cdots$$) and $$\bigcup_{n=1}^{\infty} E_n \in R$$, then:
$\mu(\bigcup_{n=1}^{\infty} E_n) = \lim_{n \to \infty} \mu(E_n)$

**Properties 4.** Continuity from Above (for decreasing sequences):
If $$\{E_n\}{n=1}^{\infty}$$ is a decreasing sequence of sets in R (meaning $$E_1 \supseteq E_2 \supseteq E_3 \supseteq \cdots$$) and $$\mu(E_1) < \infty$$, and $$\bigcap_{n=1}^{\infty} E_n \in R$$, then:
$$\mu(\bigcap_{n=1}^{\infty} E_n) = \lim_{n \to \infty} \mu(E_n)$$

These properties are crucial because they ensure that measures behave well with respect to limits of sets. The continuity from above property requires the additional condition that $$\mu(E_1) < \infty$$. No such condition is needed for continuity from below since measures are non-negative.


# Probability as a Measure

Probability is defined as a measure on a sigma alebra of sample space. When we conduct a random experiment, we begin with the sample space $\Omega$ (omega), which contains all possible outcomes of the experiment. A sigma algebra $\mathcal{F}$ contains events which are subsets of the sample space. A event is said to occur when the observation (outcome) of the random experiment belongs to the event. 

A collection of such events forms a $\sigma$-algebra (sigma-algebra) $\mathcal{F}$ of subsets of $\Omega$, which satisfies three key properties:

1. The sample space $\Omega$ itself belongs to $\mathcal{F}$
2. If an event $A$ belongs to $\mathcal{F}$, then its complement $A^c$ also belongs to $\mathcal{F}$
3. The union of any countable sequence of events in $\mathcal{F}$ also belongs to $\mathcal{F}$

Probability $P$ is defined as a measure that satisfies the following axioms:

1. Non-negativity: For any event $A \in \mathcal{F}$, $P(A) \geq 0$
2. $P(\Omega) = 1$
3. Countable additivity: For any sequence of disjoint events ${A_n}$ in $\mathcal{F}$, $P(\bigcup A_n) = \sum P(A_n)$

This measure-theoretic approach provides the mathematical framework necessary for handling both discrete and continuous probability distributions within a unified theory. It allows us to rigorously define concepts like random variables, independence, and conditional probability.


# Measurable Functions and Integration

## Measurable Functions

When working with different measure spaces, we need a way to transform elements from one space to another while preserving the measurability property. This is where measurable functions come into play.

**Definition:** Let $(X, \Sigma)$ and $(Y, \mathcal{T})$ be measurable spaces, meaning that $X$ and $Y$ are sets equipped with respective $\sigma$-algebras $\Sigma$ and $\mathcal{T}$. A function $f : X \to Y$ is said to be **measurable** if for every $E \in \mathcal{T}$ the pre-image of $E$ under $f$ is in $\Sigma$; that is, for all $E \in \mathcal{T}$:

$$f^{-1}(E) := \{x \in X \mid f(x) \in E\} \in \Sigma$$

In other words, $f$ is measurable if $\sigma(f) \subseteq \Sigma$, where $\sigma(f)$ is the $\sigma$-algebra generated by $f$.

When we want to emphasize the dependency on the $\sigma$-algebras, we write:

$$f : (X, \Sigma) \to (Y, \mathcal{T})$$

In many applications, particularly in probability and analysis, $\mathcal{T}$ is often taken as the Borel sigma algebra on $\mathbb{R}^n$, which is the sigma algebra generated by all open sets in $\mathbb{R}^n$.

### Properties of Measurable Functions

Measurable functions have several important properties that make them powerful tools in measure theory:

1. The sum and product of two measurable functions is measurable.
2. The composition (f(g(x)) of measurable functions is measurable.
3. If $f$ is measurable, then $|f|$, $\max(0,f(x))$, and $\max(0,-f(x))$ are all measurable.
4. The point-wise supremum and infimum of a sequence of measurable functions is measurable.
5. The liminf, limsup, and limit (all taken pointwise) of a sequence of measurable functions is a measurable function.

These properties ensure that measurable functions form a robust class that's closed under many common operations, making them ideal for mathematical analysis.

## Simple Functions: Building Blocks for Integration

A fundamental concept in measure theory is the simple function, which serves as a building block for defining integration.

**Definition:** Let $(X, \Sigma)$ be a measurable space. A **simple function** is a function $f:X\to\mathbb{C}$ of the form:

$$f(x)=\sum_{k=1}^{n}a_k \mathbf{1}_{A_k}(x)$$

where $A_1, \ldots, A_n \in \Sigma$ are disjoint measurable sets, $a_1, \ldots, a_n$ are real or complex numbers, and $\mathbf{1}_A$ is the indicator function of the set $A$.

Simple functions can be thought of as step functions that take only a finite number of values. They're particularly important because of the following theorem:

**Theorem:** If $f$ is a positive measurable function, then there exists an increasing sequence of simple functions converging pointwise to it.

This theorem is crucial for integration theory as it allows us to approximate any positive measurable function using simple functions, for which integration is straightforward to define.

## Integration in Measure Theory

Integration in measure theory extends the familiar Riemann integral to more general settings. We build it in stages:

### Integration of Simple Functions

Let $(X, \Sigma, \mu)$ be a measure space and $f : X \rightarrow [0, \infty]$ be a simple function of the form $f = \sum_{k=1}^{n} a_k \mathbf{1}_{A_k}$ where $A_1, \ldots, A_n \in \Sigma$ are disjoint measurable sets.

The integral of $f$ with respect to the measure $\mu$ is defined as:

$$\int_X f \, d\mu = \sum_{k=1}^{n} a_k \mu(A_k)$$

If $E \in \Sigma$, then the integral of $f$ over $E$ is defined as:

$$\int_E f \, d\mu = \int_X f \mathbf{1}_E \, d\mu = \sum_{k=1}^{n} a_k \mu(A_k \cap E)$$

This definition is intuitive: we weight each value of the function by the measure of the set on which it takes that value.

### Integration of Positive Measurable Functions

For a general positive measurable function $f: X \to [0, \infty]$, we use the approximation theorem mentioned earlier. Since we can approximate $f$ by an increasing sequence of simple functions, we define:

$$\int_X f \, d\mu = \sup_{0 \leq \phi \leq f} \int_X \phi \, d\mu$$

where the supremum is taken over all simple functions $\phi$ such that $0 \leq \phi \leq f$.

### Integration of Positive Measurable Functions

For a general positive measurable function $f: X \to [0, \infty]$, we use the approximation theorem mentioned earlier. Since we can approximate $f$ by an increasing sequence of simple functions, we define:

$$\int_X f \, d\mu = \sup_{0 \leq \phi \leq f} \int_X \phi \, d\mu$$

where the supremum is taken over all simple functions $\phi$ such that $0 \leq \phi \leq f$.

### Integration of Integrable Functions

While the definition for positive measurable functions provides a foundation, we need to extend integration to handle functions that take both positive and negative values (or even complex values). We approach this by decomposing a general function into its positive and negative parts.

For any real-valued measurable function $f$, we can define:
- The positive part: $f^+(x) = \max(f(x), 0)$
- The negative part: $f^-(x) = \max(-f(x), 0)$

Note that both $f^+$ and $f^-$ are non-negative measurable functions, and $f = f^+ - f^-$.

**Definition:** A measurable function $f: X \to \mathbb{R}$ is said to be **integrable** if both $\int_X f^+ \, d\mu < \infty$ and $\int_X f^- \, d\mu < \infty$. 

For an integrable function $f$, we define its integral as:

$$\int_X f \, d\mu = \int_X f^+ \, d\mu - \int_X f^- \, d\mu$$

This approach is sometimes called the Lebesgue integral. It's worth noting that this definition ensures that the integral is well-defined (not $\infty - \infty$) because we require both parts to be finite.

The space of all integrable functions with respect to measure $\mu$ is commonly denoted as $L^1(X, \mu)$ or simply $L^1$ when the context is clear.

#### Extension to Complex-Valued Functions

For complex-valued measurable functions $f: X \to \mathbb{C}$, we decompose $f$ into its real and imaginary parts:

$$f(x) = \text{Re}(f(x)) + i\text{Im}(f(x))$$

A complex-valued function $f$ is integrable if both its real and imaginary parts are integrable, and we define:

$$\int_X f \, d\mu = \int_X \text{Re}(f) \, d\mu + i\int_X \text{Im}(f) \, d\mu$$

#### Some Properties of the Integral

The Lebesgue integral has several important properties:

1. **Linearity**: For integrable functions $f$ and $g$ and constants $\alpha$ and $\beta$:
   $$\int_X (\alpha f + \beta g) \, d\mu = \alpha \int_X f \, d\mu + \beta \int_X g \, d\mu$$

2. **Monotonicity**: If $f$ and $g$ are integrable and $f(x) \leq g(x)$ for almost all $x \in X$, then:
   $$\int_X f \, d\mu \leq \int_X g \, d\mu$$

3. **Absolute value inequality**: If $f$ is integrable, then:
   $$\left|\int_X f \, d\mu\right| \leq \int_X |f| \, d\mu$$


## Convergence Theorems in Integration Theory

A central question in integration theory is: When can we interchange limits and integrals? That is, under what conditions does $\lim_{n\to\infty} \int f_n \, d\mu = \int \lim_{n\to\infty} f_n \, d\mu$? The following theorems provide answers to this question in various settings.

### 1. Monotone Convergence Theorem

**Theorem (Monotone Convergence):** Let $(X, \Sigma, \mu)$ be a measure space. If $\{f_n\}$ is an increasing sequence of measurable functions (i.e., $0 \leq f_1 \leq f_2 \leq \ldots$) that converges pointwise to a function $f$, then:

$$\lim_{n \to \infty} \int_X f_n \, d\mu = \int_X f \, d\mu$$

This theorem allows us to interchange the limit and integral operations when dealing with an increasing sequence of non-negative functions.

**Proof Sketch:**
1. Since $f_n \leq f$ for all $n$, we have $\int_X f_n \, d\mu \leq \int_X f \, d\mu$ by monotonicity of the integral.
2. Therefore, $\lim_{n \to \infty} \int_X f_n \, d\mu \leq \int_X f \, d\mu$.
3. For the reverse inequality, we need to show that for any simple function $s$ with $0 \leq s \leq f$, we have $\int_X s \, d\mu \leq \lim_{n \to \infty} \int_X f_n \, d\mu$.
4. The key insight is that for any $\epsilon > 0$ and any such simple function $s$, the set $\{x \in X : f_n(x) < s(x) - \epsilon\}$ decreases to the empty set as $n$ increases.
5. Using this fact and the definition of the integral for simple functions, we can establish that $\int_X f \, d\mu \leq \lim_{n \to \infty} \int_X f_n \, d\mu$.
6. Combining the inequalities from steps 2 and 5 completes the proof.

### 2. Beppo Levi's Theorem (Series Version of MCT)

**Theorem (Beppo Levi):** Let $(X, \Sigma, \mu)$ be a measure space and let $\{f_n\}$ be a sequence of non-negative measurable functions. Then:

$$\int_X \sum_{n=1}^{\infty} f_n \, d\mu = \sum_{n=1}^{\infty} \int_X f_n \, d\mu$$

This theorem is essentially an application of the Monotone Convergence Theorem to series. It states that the integral of a sum equals the sum of the integrals, provided all functions are non-negative.

**Proof Sketch:**
1. Define the partial sums $S_N = \sum_{n=1}^{N} f_n$.
2. The sequence $\{S_N\}$ is increasing and converges pointwise to $\sum_{n=1}^{\infty} f_n$.
3. By the Monotone Convergence Theorem:
   $$\int_X \sum_{n=1}^{\infty} f_n \, d\mu = \int_X \lim_{N \to \infty} S_N \, d\mu = \lim_{N \to \infty} \int_X S_N \, d\mu = \lim_{N \to \infty} \sum_{n=1}^{N} \int_X f_n \, d\mu = \sum_{n=1}^{\infty} \int_X f_n \, d\mu$$

### 3. Fatou's Lemma

**Lemma (Fatou):** Let $(X, \Sigma, \mu)$ be a measure space and let $\{f_n\}$ be a sequence of non-negative measurable functions. Then:

$$\int_X \liminf_{n \to \infty} f_n \, d\mu \leq \liminf_{n \to \infty} \int_X f_n \, d\mu$$

Fatou's Lemma essentially says that the integral of the limit inferior is less than or equal to the limit inferior of the integrals. It provides a one-sided bound when we don't have monotonicity or domination.

**Proof Sketch:**
1. For each $m$, define $g_m(x) = \inf_{n \geq m} f_n(x)$.
2. The sequence $\{g_m\}$ is increasing and converges pointwise to $\liminf_{n \to \infty} f_n(x)$.
3. By the Monotone Convergence Theorem:
   $$\int_X \liminf_{n \to \infty} f_n \, d\mu = \int_X \lim_{m \to \infty} g_m \, d\mu = \lim_{m \to \infty} \int_X g_m \, d\mu$$
4. For each $m$ and each $n \geq m$, we have $g_m \leq f_n$, so $\int_X g_m \, d\mu \leq \int_X f_n \, d\mu$.
5. Taking the infimum over all $n \geq m$ on the right side:
   $$\int_X g_m \, d\mu \leq \inf_{n \geq m} \int_X f_n \, d\mu$$
6. Taking the limit as $m \to \infty$ on both sides:
   $$\lim_{m \to \infty} \int_X g_m \, d\mu \leq \lim_{m \to \infty} \inf_{n \geq m} \int_X f_n \, d\mu = \liminf_{n \to \infty} \int_X f_n \, d\mu$$
7. Combining with step 3 completes the proof.

### 4. Dominated Convergence Theorem

**Theorem (Dominated Convergence):** Let $(X, \Sigma, \mu)$ be a measure space. If $\{f_n\}$ is a sequence of measurable functions that converges pointwise almost everywhere to a function $f$, and there exists an integrable function $g$ such that $|f_n(x)| \leq g(x)$ for all $n$ and almost all $x \in X$, then:

1. $f$ is integrable
2. $\lim_{n \to \infty} \int_X f_n \, d\mu = \int_X f \, d\mu$

This powerful theorem gives conditions under which we can interchange limits and integrals even for functions that take both positive and negative values.

**Proof Sketch:**
1. Since $|f_n(x)| \leq g(x)$ for all $n$ and $f_n \to f$ pointwise, we have $|f(x)| \leq g(x)$ almost everywhere. Since $g$ is integrable, this implies $f$ is integrable.
2. Define $h_n = g + f_n$ and $h = g + f$. Note that $h_n \geq 0$ and $h_n \to h$ pointwise.
3. Apply Fatou's Lemma to $\{h_n\}$:
   $$\int_X h \, d\mu = \int_X \liminf_{n \to \infty} h_n \, d\mu \leq \liminf_{n \to \infty} \int_X h_n \, d\mu = \liminf_{n \to \infty} \left(\int_X g \, d\mu + \int_X f_n \, d\mu\right)$$
4. This gives us $\int_X f \, d\mu \leq \liminf_{n \to \infty} \int_X f_n \, d\mu$.
5. Similarly, apply Fatou's Lemma to $\{g - f_n\}$ to get $\int_X (-f) \, d\mu \leq \liminf_{n \to \infty} \int_X (-f_n) \, d\mu$.
6. This gives us $-\int_X f \, d\mu \leq \liminf_{n \to \infty} (-\int_X f_n \, d\mu) = -\limsup_{n \to \infty} \int_X f_n \, d\mu$.
7. Therefore, $\limsup_{n \to \infty} \int_X f_n \, d\mu \leq \int_X f \, d\mu$.
8. Combining the inequalities from steps 4 and 7, we get:
   $$\limsup_{n \to \infty} \int_X f_n \, d\mu \leq \int_X f \, d\mu \leq \liminf_{n \to \infty} \int_X f_n \, d\mu$$
9. Since $\limsup \leq \liminf$ implies they are equal, we have $\lim_{n \to \infty} \int_X f_n \, d\mu = \int_X f \, d\mu$.


